{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS153 Computer Vision Eye-Tracking Dataset RF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Necessary Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the following cell the first time you run the notebook. It's just to download the necessary dependencies for the models contained in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pip\n",
    "!python -m pip install -U pip\n",
    "# Install scikit-image\n",
    "!python -m pip install -U scikit-image\n",
    "!pip install -U scikit-learn\n",
    "!python -m pip install openpyxl==3.0\n",
    "!pip install -q DataSynthesizer\n",
    "!pip install optuna\n",
    "# Current stable release for CPU and GPU\n",
    "!pip install tensorflow\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip3 install opencv-python\n",
    "!pip install dlib\n",
    "!pip install face_recognition\n",
    "!pip install deepface\n",
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the tree structure required for the machine learning models to run, the following code is provided for file hiearchy purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Necessary Parent Directories\n",
    "output_data = './output_data'\n",
    "open_cv_figures = './open_cv_figures'\n",
    "\n",
    "try: \n",
    "    os.mkdir(output_data)\n",
    "except OSError as error: \n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir(open_cv_figures) \n",
    "except OSError as error: \n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing version control on packages so that the Conda environment does not run into dependency issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "import skimage as skim\n",
    "from skimage import transform as sktsf\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "import sys\n",
    "sys.path.append('models')\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch as t\n",
    "from torchvision import transforms as tvtsf\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "!python --version\n",
    "print(\"Matplotlib: \", mpl.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Optuna: \", optuna.__version__)\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"PIL: \", Image.__version__)\n",
    "print(\"Seaborn: \", sns.__version__)\n",
    "print(\"Scikit-image: \", skim.__version__)\n",
    "print(\"Scikit-learn: \", sklearn.__version__)\n",
    "print(\"TensorFlow: \", tf.__version__)\n",
    "print(\"PyTorch: \", t.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset We're Using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaze Data for the Analysis of Attention in Feature Films\n",
    "The data set contains gaze information in response to carefully curated film clips\n",
    "\n",
    "which have been augmented via the annotation of selected high-level features. This includes:\n",
    "- 15 film clips, of duration 1 – 4 minutes each, selected to be representative of a variety of genres,\n",
    "temporal pacing, and visual styles\n",
    "- Recorded gaze behavior for 21 viewers watching those clips\n",
    "- Frame by frame annotations of high-level cinematographic features\n",
    "\n",
    "This data set is easily extendable, either with additional hand annotations or with existing\n",
    "methods from machine vision. The following sections describe the selection of candidate films, the\n",
    "identification of clips from within these films, and the high-level image features that have been\n",
    "annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the txt space separated text files into csv files so that we can read them in as Pandas dataframes\n",
    "import csv\n",
    "with open(\"./input_data/all_gazepoints_max_invalid_per_subject10%.txt\") as fin, open(\"./input_data/all_gazepoints_max_invalid_per_subject10%.csv\", \"w\") as fout:\n",
    "    o=csv.writer(fout)\n",
    "    for line in fin:\n",
    "        o.writerow(line.split())\n",
    "\n",
    "with open(\"./input_data/features_by_frame_all_clips.txt\") as fin, open(\"./input_data/features_by_frame_all_clips.csv\", \"w\") as fout:\n",
    "    o=csv.writer(fout)\n",
    "    for line in fin:\n",
    "        o.writerow(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gazepoint data into a Pandas dataframe\n",
    "gazepoint_df = pd.read_csv(\"./input_data/all_gazepoints_max_invalid_per_subject10%.csv\",encoding='utf-8')\n",
    "gazepoint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature-by-frame data into a Pandas dataframe\n",
    "feature_df = pd.read_csv(\"./input_data/features_by_frame_all_clips.csv\",encoding='utf-8')\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let us remove all columns from both of the dataframes that are not relevant to our research\n",
    "gazepoint_df = gazepoint_df.drop(columns=[\"subject\", \"eyetracker_valid\", \"in_frame\", \"subject_valid_for_clip\"])\n",
    "feature_df.drop(feature_df.columns.difference(['frame_num','shot_num', \"face\", \"faces\", \"film\"]), 1, inplace=True)\n",
    "gazepoint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us remove all entries in the feature_df that do not contain at least one face\n",
    "mask = (feature_df.face==1) | (feature_df.faces==1)\n",
    "feature_df = feature_df[mask]\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to creating the necessary file hiearchy before, use the following code to create sub-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Necessary Sub Directories\n",
    "films = feature_df.film.unique()\n",
    "root_output_data_path = \"./output_data\"\n",
    "root_open_cv_figures_path = \"./open_cv_figures/\"\n",
    "for film in films:\n",
    "    try: \n",
    "        path1 = os.path.join(root_output_data_path, film)\n",
    "        path2 = os.path.join(root_open_cv_figures_path, film)\n",
    "        os.mkdir(path1) \n",
    "        os.mkdir(path2) \n",
    "    except OSError as error: \n",
    "        print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know the frame_nums for each movie where a face appears. For each movie, let's go through the frame images so that for each frame_num, we append the corresponding filePaths to the feature_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def map_coordiantes(image):\n",
    "    file_name = Path(image).stem\n",
    "    try:\n",
    "        frame_num = int(file_name[file_name.rindex('_')+1:])\n",
    "        return frame_num\n",
    "    except ValueError as e:\n",
    "        print(f\"The folder contains a non-image: {e}\")\n",
    "# go through the clips in frame and create a dictionary where the key = frame_num, and value = filePath\n",
    "full_movie_dict = {}\n",
    "for movie in os.listdir(\"./frames\"):\n",
    "    frame_num_dict = {}\n",
    "    for image_filePath in os.listdir(f\"./frames/{movie}\"):\n",
    "        frame_num = map_coordiantes(image_filePath)\n",
    "        frame_num_dict[frame_num] = image_filePath\n",
    "    full_movie_dict[movie] = frame_num_dict\n",
    "\n",
    "feature_df = feature_df.reset_index()\n",
    "for index, row in feature_df.iterrows():\n",
    "    film = row['film']\n",
    "    df_frame_num = row['frame_num']\n",
    "    if os.path.isdir(f\"./frames/{film}\"):\n",
    "        if df_frame_num in full_movie_dict[film].keys():\n",
    "            corr_filePath = f\"./frames/{film}/{full_movie_dict[film][df_frame_num]}\"\n",
    "            feature_df[\"file_path\"] = corr_filePath\n",
    "            shutil.copy(corr_filePath, f\"./output_data/{film}\")\n",
    "    else:\n",
    "        feature_df[\"file_path\"] = None\n",
    "\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Face Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the deepFace library comes with the most advanced face detection models, it is only able to generate a similarity function between the target image and the rest of the images in a database as shown below in the example using my face and the film frames from the movie Argo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "backends = ['opencv', 'ssd', 'dlib', 'mtcnn', 'retinaface', 'mediapipe']\n",
    "deepFace = DeepFace.find(img_path = \"../Profilepicture.jpg\", db_path = \"./frames/argo/\", enforce_detection = False, model_name = models[2], detector_backend=backends[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepFace.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A horrible, non-trained face detection model that only recognizes faces for high-quality pictures such as the example image of my face below. This model does not work for movie frames as the quality of the images is not high enough to detect faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model With Horrible AS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some of the frames\n",
    "def plt_img_by_title(title):\n",
    "    _, _, files = next(os.walk(f'./frames/{title}/'))\n",
    "    file_num = len(files)\n",
    "    for i in range(1, file_num, 500):\n",
    "        n = 5 - len(str(i))\n",
    "        img = mpimg.imread(f'./frames/{title}/{title}_{\"0\"*n+str(i)}.png')\n",
    "        imgplot = plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "plt_img_by_title('argo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a FaceDetector class that employs cv2's CascadeClassifier() to detect rectange coordinates for faces for a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class FaceDetector():\n",
    "\n",
    "    def __init__(self,faceCascadePath):\n",
    "        self.faceCascade=cv2.CascadeClassifier(faceCascadePath)\n",
    "\n",
    "\n",
    "    def detect(self, image, scaleFactor=1.1,\n",
    "               minNeighbors=5,\n",
    "               minSize=(5,5)):\n",
    "        \n",
    "        #function return rectangle coordinates of faces for given image\n",
    "        rects=self.faceCascade.detectMultiScale(image,\n",
    "                                                scaleFactor=scaleFactor,\n",
    "                                                minNeighbors=minNeighbors,\n",
    "                                                minSize=minSize)\n",
    "        return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frontal face of haar cascade loaded\n",
    "frontal_cascade_path=\"/Users/staniya/Documents/HMC/SPRING2022/CS153_Computer_Vision/Final Project/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "#Detector object created\n",
    "fd=FaceDetector(frontal_cascade_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for reading and saving images using cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_my_image(image):\n",
    "    image = cv2.imread(image)\n",
    "    return np.copy(image)\n",
    "\n",
    "def save_image(film, image, image_name):\n",
    "    plt.figure(figsize=(18,15))\n",
    "    #Before showing image, bgr color order transformed to rgb order\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(f\"./open_cv_figures/{film}/{image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(image, scaleFactor, minNeighbors, minSize, film, image_name):\n",
    "    # face will detected in gray image\n",
    "    image_gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces=fd.detect(image_gray,\n",
    "                   scaleFactor=scaleFactor,\n",
    "                   minNeighbors=minNeighbors,\n",
    "                   minSize=minSize)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        #detected faces shown in color image\n",
    "        cv2.rectangle(image,(x,y),(x+w, y+h),(127, 255,0),3)\n",
    "\n",
    "    save_image(film, image, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df = feature_df.reset_index()\n",
    "# for index, row in feature_df.iterrows():\n",
    "#     film = row['film']\n",
    "#     file_path = row['file_path']\n",
    "#     if file_path is not None:\n",
    "#         image_name = os.path.basename(file_path)\n",
    "#         detect_face(image=get_my_image(file_path), \n",
    "#                     scaleFactor=1.9, \n",
    "#                     minNeighbors=3, \n",
    "#                     minSize=(30,30),\n",
    "#                     film=film,\n",
    "#                     image_name=image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Sophisticated Model That Uses cv2.CascadeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell has code structured so that for each of the films stored in the frames directory (assuming the above cells have ran properly and a Pandas dataframe has been created), apply the face detection algorithm and store the resulting file in open_cv_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.reset_index()\n",
    "for index, row in feature_df.iterrows():\n",
    "    file_path = row['file_path']\n",
    "    if file_path is not None:\n",
    "        # load the haar cascade face detector from\n",
    "        detector = cv2.CascadeClassifier(\"../haarcascade_frontalface_default.xml\")\n",
    "        # load the input image from disk, resize it, and convert it to\n",
    "        # grayscale\n",
    "        image = cv2.imread(file_path)\n",
    "        image = imutils.resize(image, width=500)\n",
    "        film_name = Path(file_path).parts[1]\n",
    "        image_name = Path(file_path).parts[2]\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect faces in the input image using the haar cascade face\n",
    "        # detector\n",
    "\n",
    "        rects = detector.detectMultiScale(gray, scaleFactor=1.05,\n",
    "                                        minNeighbors=7, minSize=(30, 30),\n",
    "                                        flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        # loop over the bounding boxes\n",
    "        for (x, y, w, h) in rects:\n",
    "            # draw the face bounding box on the image\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # show the output image\n",
    "        cv2.imwrite(f\"./open_cv_figures/{film}/{file_path}\", image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCHD Image Manipulation Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions necesary to preproces the image data so that they can be scaled and rendered by PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize(img):\n",
    "    if opt.caffe_pretrain:\n",
    "        img = img + (np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1))\n",
    "        return img[::-1, :, :]\n",
    "    return (img * 0.225 + 0.45).clip(min=0, max=1) * 255\n",
    "\n",
    "def pytorch_normalze(img):\n",
    "    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(t.from_numpy(img))\n",
    "    return img.numpy()\n",
    "\n",
    "def caffe_normalize(img):\n",
    "    img = img[[2, 1, 0], :, :]  # RGB-BGR\n",
    "    img = img * 255\n",
    "    mean = np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1)\n",
    "    img = (img - mean).astype(np.float32, copy=True)\n",
    "    return img\n",
    "\n",
    "def preprocess(img, min_size=600, max_size=1000):   \n",
    "    \"\"\" Function to preprocess the input image. \n",
    "    \n",
    "    Scales the input image in such a manner that the shorter side of the \n",
    "    image is converted to the size equal to min_size. \n",
    "    Also normalizes the input image. \n",
    "\n",
    "    Args: \n",
    "        img: Input image that is to be preprocessed. \n",
    "        min_size: size to which the smaller side of the image is to be \n",
    "                    converted. \n",
    "        max_size: size to which the larger side of the image is to be \n",
    "                    converted. \n",
    "    \"\"\"\n",
    "    C, H, W = img.shape\n",
    "    scale1 = min_size / min(H, W)\n",
    "    scale2 = max_size / max(H, W)\n",
    "    scale = min(scale1, scale2)\n",
    "    img = img / 255.\n",
    "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect')\n",
    "    # both the longer and shorter should be less than\n",
    "    # max_size and min_size\n",
    "    if opt.caffe_pretrain:\n",
    "        normalize = caffe_normalize\n",
    "    else:\n",
    "        normalize = pytorch_normalze\n",
    "    return normalize(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class function that processes the frame images such that they can serve as inputs for the FCHD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeGazeDataset:\n",
    "    \n",
    "    def __init__(self, dl):\n",
    "        self.datalist = dl\n",
    "    \n",
    "    def get_example(self, idx):\n",
    "        data_obj = self.datalist[idx]\n",
    "        img_path = data_obj.path\n",
    "        n_boxs = data_obj.n_boxs\n",
    "        bboxs = data_obj.bboxs\n",
    "        print(bboxs)\n",
    "        img, scale_w, scale_h = self.read_image(img_path)\n",
    "        # scale_bboxs = []\n",
    "        for i in range(n_boxs):\n",
    "            # ymin,xmin,ymax,xmax = bboxs[i,:]\n",
    "            # ymin = bbox[0]*scale_h\n",
    "            # bbox[1] = bbox[1]*scale_w\n",
    "            # bbox[2] = bbox[2]*scale_h\n",
    "            # bbox[3] = bbox[3]*scale_w\n",
    "            \n",
    "            bboxs[i,0] = bboxs[i,0]*scale_h\n",
    "            bboxs[i,1] = bboxs[i,1]*scale_w\n",
    "            bboxs[i,2] = bboxs[i,2]*scale_h\n",
    "            bboxs[i,3] = bboxs[i,3]*scale_w\n",
    "\n",
    "            # scale_bboxs.append(bbox)\n",
    "        return img, bboxs, n_boxs      \n",
    "\n",
    "    def read_image(self, path, dtype=np.float32):\n",
    "        f = Image.open(path)\n",
    "        # w_O, h_O = f.size\n",
    "        W_o, H_o = f.size\n",
    "        # print \"Height: %s\" %(H_o)\n",
    "        # print \"Width: %s\" %(W_o)\n",
    "        f = f.resize((640,480), Image.ANTIALIAS)\n",
    "        W_n, H_n = f.size        \n",
    "        # Convert to RGB\n",
    "\n",
    "        scale_w = W_n / W_o\n",
    "        scale_h = H_n / H_o\n",
    "\n",
    "\n",
    "        f.convert('RGB')\n",
    "        # Convert to a numpy array\n",
    "        img = np.asarray(f, dtype=np.float32)\n",
    "        # _, h_N, w_N = img.shape\n",
    "        # Transpose the final image array i.e. C, H, W\n",
    "        return img.transpose((2, 0, 1)), scale_w, scale_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_df = pd.read_csv(\"./input_data/features_by_frame_all_clips.csv\",encoding='utf-8')\n",
    "NUM_ßROWS, NUM_COLS = feature_df.shape[0], feature_df.shape[1]\n",
    "Y_col = 'face'\n",
    "X_cols = feature_df.loc[:, feature_df.columns != Y_col].columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "train_test_split(feature_df[X_cols], feature_df[Y_col],test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_op, X_val_op, y_train_op, y_val_op = X_train.copy(), X_val.copy(), y_train.copy(), y_val.copy()\n",
    "\n",
    "print(\"X_train.shape = \", X_train.shape, \" \\t y_train.shape = \", y_train.shape)\n",
    "print(\"X_val.shape = \", X_val.shape, \" \\t y_val.shape = \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following k-fold cross validatoin fails as the above cells do not have an appropriate case for handling NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "best_depth = 1\n",
    "best_num_trees = 42\n",
    "\n",
    "for ntrees in range(50,450,100):  \n",
    "    for d in range(1,20): \n",
    "        rforest_model = ensemble.RandomForestClassifier(max_depth=d, \n",
    "                                                        n_estimators=ntrees,\n",
    "                                                        random_state=42)\n",
    "        cv_scores = cross_val_score( rforest_model, X_train, y_train, cv=5 )\n",
    "        average_cv_accuracy = cv_scores.mean()  \n",
    "        if average_cv_accuracy >= best_accuracy: \n",
    "            best_accuracy = average_cv_accuracy\n",
    "            best_depth = d\n",
    "            best_num_trees = ntrees\n",
    "        print(f\"depth: {d:2d} num_trees : {ntrees:3d} average_cv_accuracy: {average_cv_accuracy:7.4f}\")\n",
    "\n",
    "print()\n",
    "print(f\"best_depth: {best_depth}, best_num_trees {best_num_trees}, best_accuracy{best_accuracy}\")\n",
    "\n",
    "rforest_model_cv = ensemble.RandomForestClassifier(max_depth=best_depth, n_estimators=best_num_trees) \n",
    "\n",
    "rforest_model_cv.fit(X_train, y_train) \n",
    "\n",
    "y_pred = rforest_model_cv.predict(X_val)\n",
    "\n",
    "trainaccuracy_random_forest = rforest_model_cv.score(X_train, y_train)\n",
    "print('TrainAccuracy: {}'.format(trainaccuracy_random_forest))\n",
    "\n",
    "accuracy_random_forest = accuracy_score(y_val, y_pred)\n",
    "print('Accuracy: {}'.format(accuracy_random_forest))\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "#confusion matrix\n",
    "labels = sorted(feature_df['90day-mortality'].unique())\n",
    "ax = sns.heatmap(\n",
    "confusion_matrix(y_val, y_pred),\n",
    "annot=True,\n",
    "xticklabels=labels,\n",
    "yticklabels=labels\n",
    ")\n",
    "ax.set(xlabel='true label', ylabel='predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "  bootstrap = trial.suggest_categorical('bootstrap',['True','False'])\n",
    "  max_depth = trial.suggest_int('max_depth', 1, 1000)\n",
    "  max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt','log2'])\n",
    "  max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1,1000)\n",
    "  n_estimators =  trial.suggest_int('n_estimators', 1, 1000)\n",
    "  min_samples_split = trial.suggest_int('min_samples_split',2,5)\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf',1,10)\n",
    "\n",
    "  regr = ensemble.RandomForestClassifier(\n",
    "      bootstrap = bootstrap, criterion = criterion,\n",
    "      max_depth = max_depth, max_features = max_features,\n",
    "      max_leaf_nodes = max_leaf_nodes,n_estimators = n_estimators,\n",
    "      min_samples_split = min_samples_split,min_samples_leaf = min_samples_leaf,\n",
    "      n_jobs=2)\n",
    "\n",
    "  score = cross_val_score(regr, X_train_op, y_train_op, cv=5, scoring=\"r2\")\n",
    "  r2_mean = score.mean()\n",
    "  return r2_mean\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "\n",
    "rforest_model_post_optuna = ensemble.RandomForestClassifier(bootstrap = trial.params['bootstrap'], criterion = trial.params['criterion'],\n",
    "                                     max_depth = trial.params['max_depth'], max_features = trial.params['max_features'],\n",
    "                                     max_leaf_nodes = trial.params['max_leaf_nodes'],n_estimators = trial.params['n_estimators'],\n",
    "                                     min_samples_split = trial.params['min_samples_split'],min_samples_leaf = trial.params['min_samples_leaf'],                                     \n",
    "                                     n_jobs=2)\n",
    "\n",
    "rforest_model_post_optuna.fit(X_train_op, y_train_op) \n",
    "\n",
    "y_pred_op = rforest_model_post_optuna.predict(X_val_op)\n",
    "\n",
    "trainaccuracy_random_forest_op = rforest_model_post_optuna.score(X_train_op, y_train_op)\n",
    "print('TrainAccuracy: {}'.format(trainaccuracy_random_forest_op))\n",
    "\n",
    "accuracy_random_forest_op = accuracy_score(y_val_op, y_pred_op)\n",
    "print('Accuracy: {}'.format(accuracy_random_forest_op))\n",
    "\n",
    "print(classification_report(y_val_op, y_pred_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accuracy_random_forest_op > accuracy_random_forest:\n",
    "  print(\"Optuna had the higher prediciton accuracy\")\n",
    "  feature_importances = rforest_model_post_optuna.feature_importances_\n",
    "else:\n",
    "  print(\"Cross Validation had the higher prediction accuracy\")\n",
    "  feature_importances = rforest_model_cv.feature_importances_\n",
    "\n",
    "feature_importances_dict = {}\n",
    "\n",
    "for i, importance in enumerate(feature_importances):\n",
    "  perc = importance * 100\n",
    "  feature_importances_dict[feature_df.columns[i]] = perc \n",
    "\n",
    "sorted_dict = {}\n",
    "sorted_keys = sorted(feature_importances_dict, key=feature_importances_dict.get)\n",
    "for w in sorted_keys:\n",
    "    sorted_dict[w] =  feature_importances_dict[w]\n",
    "  \n",
    "for keys in sorted_dict.keys():\n",
    "  print(f\"フィーチャー名：{keys:>12s}は出力データの結果に{sorted_dict[keys]:>7.2f}%関与しています\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
